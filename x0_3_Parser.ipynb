{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsers are one of the core techniques in fuzzing. You need parsers to take a structured input apart, and reuse the parts in other inputs without affecting the validity of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "```python\n",
    "import parser as P\n",
    "my_grammar = {'<start>': [['1', '<A>'],\n",
    "                          ['2']\n",
    "                         ],\n",
    "              '<A>'    : [['a']]}\n",
    "my_parser = P.LL1Parser(my_grammar)\n",
    "for tree in my_parser.parse_on(text='1a', start_symbol='<start>'):\n",
    "    print(P.format_parsetree(tree))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, as per traditional implementations,\n",
    "there can only be one expansion rule for the `<start>` symbol. We work around\n",
    "this restriction by simply constructing as many charts as there are expansion\n",
    "rules, and returning all parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_1_Grammars as grammars\n",
    "import ipynb.fs.full.Railroads as diagrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagrams.syntax_diagram(grammars.EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {\n",
    "    '<start>': [['<expr>']],\n",
    "    '<expr>': [\n",
    "        ['<term>', '+', '<expr>'],\n",
    "        ['<term>', '-', '<expr>'],\n",
    "        ['<term>']],\n",
    "    '<term>': [\n",
    "        ['<fact>', '*', '<term>'],\n",
    "        ['<fact>', '/', '<term>'],\n",
    "        ['<fact>']],\n",
    "    '<fact>': [\n",
    "        ['<digits>'],\n",
    "        ['(','<expr>',')']],\n",
    "    '<digits>': [\n",
    "        ['<digit>','<digits>'],\n",
    "        ['<digit>']],\n",
    "    '<digit>': [[\"%s\" % str(i)] for i in range(10)],\n",
    "}\n",
    "START = '<start>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diagrams.syntax_diagram(grammars.BEXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "An LL(1) parser executes the following steps for parsing:\n",
    "\n",
    "The idea behind a simple $LL(1)$ recognizer is that, you try to unify the string you want to match with the corresponding key in the grammar. If the key is not present in the grammar, it is a literal, which needs to be matched with string equality. If the key is present in the grammar, get the corresponding productions (rules) for that key, and start unifying each rule one by one on the string to be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import functools\n",
    "\n",
    "class LL1Parser:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def unify_key(self, key, text, at=0):\n",
    "        if not utils.is_nt(key):\n",
    "            if text[at:].startswith(key): return (at + len(key), (key, [])) \n",
    "            else: return (at, None)\n",
    "        rules = self.grammar[key]\n",
    "        for rule in rules:\n",
    "            l, res = self.unify_rule(rule, text, at)\n",
    "            if res is not None: return l, (key, res)\n",
    "        return (0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unifying rules, the idea is similar. We take each token in the rule, and try to unify that token with the string to be matched. We rely on unify_key for doing the unification of the token. if the unification fails, we return empty handed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LL1Parser(LL1Parser):\n",
    "    def unify_rule(self, parts, text, tfrom):\n",
    "        results = []\n",
    "        for part in parts:\n",
    "            tfrom, res = self.unify_key(part, text, tfrom)\n",
    "            if res is None: return tfrom, None\n",
    "            results.append(res)\n",
    "        return tfrom, results\n",
    "\n",
    "    def parse_on(self, text, start_symbol):\n",
    "        till, result = self.unify_key(start_symbol, text, 0)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    small_grammar = {'<start>': [['1', '<A>'],\n",
    "                              ['2']\n",
    "                             ],\n",
    "                  '<A>'    : [['a']]}\n",
    "    my_parser = LL1Parser(small_grammar)\n",
    "    for tree in my_parser.parse_on(text='1a', start_symbol='<start>'):\n",
    "        utils.display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    my_parser = LL1Parser(grammars.EXPR_GRAMMAR)\n",
    "    tree = list(my_parser.parse_on(text='(8/3)*49', start_symbol='<start>'))[0]\n",
    "    utils.display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStats:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "        self.vars = {}\n",
    "\n",
    "    def rule_str(self, rule):\n",
    "        return ''.join(rule)\n",
    "\n",
    "    def to_rule_str(self, tree):\n",
    "        key, children = tree\n",
    "        return ''.join([c[0] for c in children])\n",
    "\n",
    "    def process_parsetree(self, parse_tree):\n",
    "        key, children = parse_tree\n",
    "        if not utils.is_nt(key): return\n",
    "        rule_str = self.to_rule_str(parse_tree)\n",
    "        if key not in self.vars:\n",
    "            self.vars[key] = {self.rule_str(r): 0 for r in self.grammar[key]}\n",
    "        key_vars = self.vars[key]\n",
    "        if rule_str not in key_vars:\n",
    "            key_vars[rule_str] = 1\n",
    "        else:\n",
    "            key_vars[rule_str] += 1\n",
    "        for child in children:\n",
    "            self.process_parsetree(child)\n",
    "        return self.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    stats = InputStats(grammars.EXPR_GRAMMAR)\n",
    "    stats.process_parsetree(tree)\n",
    "    for k in stats.vars:\n",
    "        print(k, stats.vars[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Depth Based Rule Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStats(InputStats):\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "        self.vars = {}  # Original counts without depth\n",
    "        self.vars_with_depth = {}  # Counts with depth\n",
    "    \n",
    "    def process_parsetree_with_depth(self, parse_tree, depth=0):\n",
    "        key, children = parse_tree        \n",
    "        if not utils.is_nt(key): return\n",
    "\n",
    "        rule_str = self.to_rule_str(parse_tree)\n",
    "        if key not in self.vars_with_depth:\n",
    "            self.vars_with_depth[key] = {self.rule_str(r):[] for r in self.grammar[key]}\n",
    "        rule_depths = self.vars_with_depth[key][rule_str]\n",
    "\n",
    "        # Track with depth\n",
    "        rule_depths.append(depth)\n",
    "\n",
    "        for child in children:\n",
    "            self.process_parsetree_with_depth(child, depth + 1)\n",
    "        return self.vars_with_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import statistics\n",
    "    stats = InputStats(grammars.EXPR_GRAMMAR)\n",
    "    stats.process_parsetree_with_depth(tree)\n",
    "    for k in stats.vars_with_depth:\n",
    "        print(k)\n",
    "        kv = stats.vars_with_depth[k]\n",
    "        for v in kv:\n",
    "            print(\"- \", v, kv[v])\n",
    "            if len(kv[v]) > 0:\n",
    "                print('  mean depth:', statistics.mean(kv[v]))\n",
    "            if len(kv[v]) > 1:\n",
    "                print('  stdev (%0.2f)' % statistics.stdev(kv[v]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position Based Rule Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputStats(InputStats):\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "        self.vars = {}  # Original counts without depth\n",
    "        self.vars_with_depth = {}  # Counts with depth\n",
    "        self.vars_with_position = {}\n",
    "    \n",
    "    def process_parsetree_with_position(self, parse_tree, position=None):\n",
    "        key, children = parse_tree        \n",
    "        # Position advances when a terminal is consumed.\n",
    "        if position is None: position = [0]\n",
    "        if not utils.is_nt(key):\n",
    "            position[0] += 1\n",
    "            return\n",
    "\n",
    "        rule_str = self.to_rule_str(parse_tree)\n",
    "        if key not in self.vars_with_position:\n",
    "            self.vars_with_position[key] = {self.rule_str(r):[] for r in self.grammar[key]}\n",
    "        rule_pos = self.vars_with_position[key][rule_str]\n",
    "\n",
    "        # Track with position\n",
    "        rule_pos.append(position[0])\n",
    "\n",
    "        for child in children:\n",
    "            self.process_parsetree_with_position(child, position)\n",
    "        return self.vars_with_position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    stats = InputStats(grammars.EXPR_GRAMMAR)\n",
    "    stats.process_parsetree_with_position(tree)\n",
    "    for k in stats.vars_with_position:\n",
    "        print(k)\n",
    "        kv = stats.vars_with_position[k]\n",
    "        for v in kv:\n",
    "            print(\"- \", v, kv[v])\n",
    "            if len(kv[v]) > 0:\n",
    "                print('  mean pos:', statistics.mean(kv[v]))\n",
    "            if len(kv[v]) > 1:\n",
    "                print('  stdev (%0.2f)' % statistics.stdev(kv[v]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arborist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, tree):\n",
    "        self.tree = tree\n",
    "\n",
    "    def to_str(self):\n",
    "        return utils.tree_to_str(self.tree)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.to_str()\n",
    "\n",
    "    def count_tokens(self, tree, pos=None):\n",
    "        key, children = tree\n",
    "        if pos is None: pos = [0]\n",
    "        if not utils.is_nt(key):\n",
    "            pos[0] += 1\n",
    "        for c in children:\n",
    "            self.count_tokens(c, pos)\n",
    "        return pos[0]\n",
    "\n",
    "    def path(self, path, tree=None, pos=0):\n",
    "        fst, *rest = path\n",
    "        if tree is None: tree = self.tree\n",
    "        children = tree[1]\n",
    "        for i in range(fst):\n",
    "            pos += self.count_tokens(children[i])\n",
    "        if not rest:\n",
    "            return pos, Tree(children[fst])\n",
    "        return self.path(rest, children[fst], pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    target = tree[1][0][1][0][1][0][1]\n",
    "    utils.display_tree(target[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    t = Tree(tree)\n",
    "    print(t.to_str())\n",
    "    print('Tokens:', t.count_tokens(tree))\n",
    "    pos, t_t = t.path([0, 0, 0, 1])\n",
    "    print('pos:', pos)\n",
    "    utils.display_tree(t_t.tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    subtree = list(my_parser.parse_on(text='2+1', start_symbol='<expr>'))[0]\n",
    "    utils.display_tree(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    target[1] = subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    utils.display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(utils.tree_to_str(tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    newtree = list(my_parser.parse_on(text='18439249', start_symbol='<start>'))[0]\n",
    "    utils.display_tree(newtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to parse more grammar varieties? For example, the following grammar describing the same language will not be parsable by `LL1Parser`.\n",
    "```\n",
    "grammar = {\n",
    "    '<start>': [['<expr>']],\n",
    "    '<expr>': [\n",
    "        ['<expr>', '+', '<expr>'],\n",
    "        ['<expr>', '-', '<expr>'],\n",
    "        ['<expr>', '*', '<expr>'],\n",
    "        ['<expr>', '/', '<expr>'],\n",
    "        ['(','<expr>',')']],\n",
    "        ['<digits>']],\n",
    "    '<digits>': [\n",
    "        ['<digit>','<digits>'],\n",
    "        ['<digit>']],\n",
    "    '<digit>': [[\"%s\" % str(i)] for i in range(10)],\n",
    "}\n",
    "START = '<start>'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of such grammars, we can use one of the general context-free parsers. These include\n",
    "* Earley parser (in this repository)\n",
    "* GLL parser\n",
    "* GLR parser\n",
    "* CYK parser\n",
    "* Valiant parser\n",
    "  and so on.\n",
    "\n",
    "The tradeoff is that each of these parsers are costly when compared to the simple LL1Parser ($O(N^3)$ or beyond compared to O(N) for LL1Parser.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
