{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsers are one of the core techniques in fuzzing. You need parsers to take a structured input apart, and reuse the parts in other inputs without affecting the validity of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "```python\n",
    "import parser as P\n",
    "my_grammar = {'<start>': [['1', '<A>'],\n",
    "                          ['2']\n",
    "                         ],\n",
    "              '<A>'    : [['a']]}\n",
    "my_parser = P.LL1Parser(my_grammar)\n",
    "for tree in my_parser.parse_on(text='1a', start_symbol='<start>'):\n",
    "    print(P.format_parsetree(tree))\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, as per traditional implementations,\n",
    "there can only be one expansion rule for the `<start>` symbol. We work around\n",
    "this restriction by simply constructing as many charts as there are expansion\n",
    "rules, and returning all parse trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = {\n",
    "    '<start>': [['<expr>']],\n",
    "    '<expr>': [\n",
    "        ['<term>', '+', '<expr>'],\n",
    "        ['<term>', '-', '<expr>'],\n",
    "        ['<term>']],\n",
    "    '<term>': [\n",
    "        ['<fact>', '*', '<term>'],\n",
    "        ['<fact>', '/', '<term>'],\n",
    "        ['<fact>']],\n",
    "    '<fact>': [\n",
    "        ['<digits>'],\n",
    "        ['(','<expr>',')']],\n",
    "    '<digits>': [\n",
    "        ['<digit>','<digits>'],\n",
    "        ['<digit>']],\n",
    "    '<digit>': [[\"%s\" % str(i)] for i in range(10)],\n",
    "}\n",
    "START = '<start>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "An LL(1) parser executes the following steps for parsing:\n",
    "\n",
    "The idea behind a simple $LL(1)$ recognizer is that, you try to unify the string you want to match with the corresponding key in the grammar. If the key is not present in the grammar, it is a literal, which needs to be matched with string equality. If the key is present in the grammar, get the corresponding productions (rules) for that key, and start unifying each rule one by one on the string to be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import functools\n",
    "\n",
    "class LL1Parser:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def unify_key(self, key, text, at=0):\n",
    "        if not utils.is_nt(key):\n",
    "            if text[at:].startswith(key): return (at + len(key), (key, [])) \n",
    "            else: return (at, None)\n",
    "        rules = self.grammar[key]\n",
    "        for rule in rules:\n",
    "            l, res = self.unify_rule(rule, text, at)\n",
    "            if res is not None: return l, (key, res)\n",
    "        return (0, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For unifying rules, the idea is similar. We take each token in the rule, and try to unify that token with the string to be matched. We rely on unify_key for doing the unification of the token. if the unification fails, we return empty handed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LL1Parser(LL1Parser):\n",
    "    def unify_rule(self, parts, text, tfrom):\n",
    "        results = []\n",
    "        for part in parts:\n",
    "            tfrom, res = self.unify_key(part, text, tfrom)\n",
    "            if res is None: return tfrom, None\n",
    "            results.append(res)\n",
    "        return tfrom, results\n",
    "\n",
    "    def parse_on(self, text, start_symbol):\n",
    "        till, result = self.unify_key(start_symbol, text, 0)\n",
    "        yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_grammar = {'<start>': [['1', '<A>'],\n",
    "                          ['2']\n",
    "                         ],\n",
    "              '<A>'    : [['a']]}\n",
    "my_parser = LL1Parser(small_grammar)\n",
    "for tree in my_parser.parse_on(text='1a', start_symbol='<start>'):\n",
    "    utils.display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parser = LL1Parser(grammar)\n",
    "tree = list(my_parser.parse_on(text='(8/3)*49', start_symbol='<start>'))[0]\n",
    "utils.display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = tree[1][0][1][0][1][0][1]\n",
    "utils.display_tree(target[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subtree = list(my_parser.parse_on(text='2+1', start_symbol='<expr>'))[0]\n",
    "utils.display_tree(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target[1] = subtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.tree_to_str(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to parse more grammar varieties? For example, the following grammar describing the same language will not be parsable by `LL1Parser`.\n",
    "```\n",
    "grammar = {\n",
    "    '<start>': [['<expr>']],\n",
    "    '<expr>': [\n",
    "        ['<expr>', '+', '<expr>'],\n",
    "        ['<expr>', '-', '<expr>'],\n",
    "        ['<expr>', '*', '<expr>'],\n",
    "        ['<expr>', '/', '<expr>'],\n",
    "        ['(','<expr>',')']],\n",
    "        ['<digits>']],\n",
    "    '<digits>': [\n",
    "        ['<digit>','<digits>'],\n",
    "        ['<digit>']],\n",
    "    '<digit>': [[\"%s\" % str(i)] for i in range(10)],\n",
    "}\n",
    "START = '<start>'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of such grammars, we can use one of the general context-free parsers. These include\n",
    "* Earley parser (in this repository)\n",
    "* GLL parser\n",
    "* GLR parser\n",
    "* CYK parser\n",
    "* Valiant parser\n",
    "  and so on.\n",
    "\n",
    "The tradeoff is that each of these parsers are costly when compared to the simple LL1Parser ($O(N^3)$ or beyond compared to O(N) for LL1Parser.)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
