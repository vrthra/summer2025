{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The L* Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import enum\n",
    "import src.utils as utils\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_3_EarleyParser as parser\n",
    "import ipynb.fs.full.x0_2_GrammarFuzzer as fuzzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definitions\n",
    "\n",
    "- Input symbol: A single symbol that is consumed by the machine which can move it from one state to another. The set of such symbols is called an alphabet, and is represented by $A$\n",
    "- Membership query: A string that is passed to the blackbox. The blackbox answers yes or no.\n",
    "- Equivalence query: A grammar that is passed to the teacher as a hypothesis of what the target language is. The teacher answers yes or a counter example that behaves differently on the blackbox and the hypothesis grammar.\n",
    "- Prefix closed: a set is prefix closed if all prefixes of any of its elements are also in the same set.\n",
    "- Suffix closed: a set is suffix closed if all suffixes of any of its elements are also in the same set.\n",
    "- Observation table: A table whose rows correspond to the candidate states. The rows are made up of prefix strings that can reach given states — commonly represented as $S$, but here we will denote these by $P$ for prefixes — and the columns are made up of suffix strings that serves to distinguish these states — commonly expressed as E for extensions, but we will use to denote suffixes here. The table contains auxiliary rows $ p \\in P$ that extends each item with each alphabet $a \\in A$ as we discuss later in closedness. This table defines the language inferred by the algorithm. The contents of the table are the answers from the oracle on a string composed of the row and column labels — prefix + suffix. That is $T[s,e] = O(s.e)$. The table has two properties: closedness and consistency. If these are not met at any time, we take to resolve it.\n",
    "- The state: A state in the DFA is represented by a prefix in the observation table, and is named by the pattern of 1s and 0s in the cell contents. We represent a state corresponding the prefix $p$ as $[p]$ \n",
    "- Closedness of the observation table means that for each $p \\in P$ and each $a \\in A$ , the state represented by the auxiliary row $[p.a]$ (i.e., its contents) exists in $P$. That is, there is some $p' \\in P$ such that $[p.a] = [p']$. The idea is that, the state $[p]$ corresponding to accepts alphabet $a$ and transitions to the state $[p']$, and $p'$ must be in the main set of rows $P$.\n",
    "- Consistency of the observation table means that if two prefixes represents the same state (i.e. the contents of two rows are equal), that is $[p1] = [p2]$ then $[p1.a] = [p2.a]$ for all alphabets. The idea is that if two prefixes reach the state, then when fed any alphabet, both prefixes should transition to the same next state (represented by the pattern produced by the suffixes).\n",
    "The candidate states $P$ is prefix closed, while the set of suffixes S is suffix closed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the observation table with the alphabet. We keep the table itself as an internal dict `_T`. We also keep the prefixes in `P` and suffixes in `S`. We initialize the set of prefixes `P` to be $\\epsilon$ and the set of suffixes `S` also to be $\\epsilon$. We also add a few utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTable:\n",
    "    def __init__(self, alphabet):\n",
    "        self._T, self.P, self.S, self.A = {}, [''], [''], alphabet\n",
    "\n",
    "    def cell(self, v, e): return self._T[v][e]\n",
    "\n",
    "    def state(self, p):\n",
    "        return '<%s>' % ''.join([str(self.cell(p,s)) for s in self.S])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the observation table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    alphabet = list('abcdefgh')\n",
    "    o = ObservationTable(alphabet)\n",
    "    o._T = {p:{'':1, 'a':0, 'ba':1, 'cba':0} for p in alphabet}\n",
    "    print(o.cell('a', 'ba'))\n",
    "    print(o.state('a'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Table to Grammar\n",
    "\n",
    "Given the observation table, we can recover the grammar from this table (corresponding to the DFA). The unique cell contents of rows are states. In many cases, multiple rows may correspond to the same state (as the cell contents are the same). \n",
    "The *start state* is given by the state that correspond to the $\\epsilon$ row.\n",
    "\n",
    "A state is accepting if it on query of $ \\epsilon $ i.e. `''`, it returns 1.\n",
    " \n",
    "The formal notations are as follows. The notation $ [p] $ means the state corresponding to the prefix $ p $. The notation $ [[p,s]] $ means the result of oracle for the prefix $ p $ and the suffix $ s $. The notation $ [p](a) $ means the state obtained by feeding the input symbol $ a $ to the state $ [p] $. We take the first prefix that resulted in a particular state as its *access prefix*, and we denote the access prefix of a state $ s $ by $ \\lfloor{}s\\rfloor $ (this is not used in this post). The following is the DFA from our table.\n",
    "\n",
    "* states: $ Q = {[p] : p \\in P} $\n",
    "* start state: $ q0 = [\\epsilon] $\n",
    "* transition function: $ [p](a) \\rightarrow [p.a] $\n",
    "* accepting state: $ F = {[p] : p \\in P : [[p,\\epsilon]] = 1} $\n",
    "\n",
    "For constructing the grammar from the table, we first identify all distinguished states. Next, we identify the start state, followed by\n",
    "accepting states. Finally, we connect states together with transitions between them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTable(ObservationTable):\n",
    "    def table_to_grammar(self):\n",
    "        # Step 1: identify all distinguished states.\n",
    "        prefix_to_state = {}  # Mapping from row string to state ID\n",
    "        states = {}\n",
    "        grammar = {}\n",
    "        for p in self.P:\n",
    "            stateid = self.state(p)\n",
    "            if stateid not in states: states[stateid] = []\n",
    "            states[stateid].append(p)\n",
    "            prefix_to_state[p] = stateid\n",
    "\n",
    "        for stateid in states: grammar[stateid] = []\n",
    "\n",
    "        # Step 2: Identify the start state, which corresponds to epsilon row\n",
    "        start_nt = prefix_to_state['']\n",
    "\n",
    "        # Step 3: Identify the accepting states\n",
    "        accepting = [prefix_to_state[p] for p in self.P if self.cell(p,'') == 1]\n",
    "        if not accepting: return {'<start>': []}, '<start>'\n",
    "        for s in accepting: grammar[s] = [['<_>']]\n",
    "        grammar['<_>'] = [[]]\n",
    "\n",
    "        # Step 4: Create the transition function\n",
    "        for sid1 in states:\n",
    "            first_such_row = states[sid1][0]\n",
    "            for a in self.A:\n",
    "                sid2 = self.state(first_such_row + a)\n",
    "                grammar[sid1].append([a, sid2])\n",
    "\n",
    "        return grammar, start_nt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    alphabet = list('ab')\n",
    "    o = ObservationTable(alphabet)\n",
    "    o._T = {'':    {'': 0, 'a': 1},\n",
    "            'a':   {'': 1, 'a': 0},\n",
    "            'b':   {'': 0, 'a': 0},\n",
    "            'aa':  {'': 0, 'a': 0},\n",
    "            'ab':  {'': 0, 'a': 0},\n",
    "            'ba':  {'': 0, 'a': 0},\n",
    "            'bb':  {'': 0, 'a': 0},\n",
    "            'baa': {'': 0, 'a': 0},\n",
    "            'bab': {'': 0, 'a': 0}}\n",
    "    P = [k for k in o._T]\n",
    "    S = [k for k in o._T['']]\n",
    "    o.P, o.S = P, S\n",
    "    g, s = o.table_to_grammar()\n",
    "    print('start: ', s)\n",
    "    for k in g:\n",
    "        print(k)\n",
    "        for r in g[k]:\n",
    "            print(\" | \", r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove infinite loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def symbol_cost(grammar, symbol, seen, cache):\n",
    "    if symbol in seen: return float('inf')\n",
    "    lst = []\n",
    "    for rule in grammar.get(symbol, []):\n",
    "        if symbol in cache and str(rule) in cache[symbol]:\n",
    "            lst.append(cache[symbol][str(rule)])\n",
    "        else:\n",
    "            lst.append(expansion_cost(grammar, rule, seen | {symbol}, cache))\n",
    "    v = min(lst, default=0)\n",
    "    return v\n",
    "\n",
    "# A rule costs as much as the cost of expansion of the most costliest symbol\n",
    "# in that rule + 1.\n",
    "\n",
    "def expansion_cost(grammar, tokens, seen, cache):\n",
    "    return max((symbol_cost(grammar, token, seen, cache)\n",
    "                for token in tokens if token in grammar), default=0) + 1\n",
    "\n",
    "def compute_cost(grammar):\n",
    "    rule_cost = {}\n",
    "    for k in grammar:\n",
    "        rule_cost[k] = {}\n",
    "        for rule in grammar[k]:\n",
    "            rule_cost[k][str(rule)] = expansion_cost(grammar, rule, set(), rule_cost)\n",
    "    return rule_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTable(ObservationTable):\n",
    "    def remove_infinite_loops(self, g, start):\n",
    "        rule_cost = compute_cost(g)\n",
    "        remove_keys = []\n",
    "        for k in rule_cost:\n",
    "            if k == start: continue\n",
    "            res = [rule_cost[k][r] for r in rule_cost[k]\n",
    "                   if rule_cost[k][r] != float('inf')]\n",
    "            if not res: remove_keys.append(k)\n",
    "\n",
    "        cont = True\n",
    "        while cont:\n",
    "            cont = False\n",
    "            new_g = {}\n",
    "            for k in g:\n",
    "                if k in remove_keys: continue\n",
    "                new_g[k] = []\n",
    "                for r in g[k]:\n",
    "                    if [t for t in r if t in remove_keys]: continue\n",
    "                    new_g[k].append(r)\n",
    "                if not new_g[k]:\n",
    "                    if k == start: continue\n",
    "                    remove_keys.append(k)\n",
    "                    cont = True\n",
    "        return new_g, start\n",
    "\n",
    "class ObservationTable(ObservationTable):\n",
    "    def grammar(self):\n",
    "        g, s = self.table_to_grammar()\n",
    "        return self.remove_infinite_loops(g, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    o = ObservationTable(alphabet)\n",
    "    o._T = {'':    {'': 0, 'a': 1},\n",
    "            'a':   {'': 1, 'a': 0},\n",
    "            'b':   {'': 0, 'a': 0},\n",
    "            'aa':  {'': 0, 'a': 0},\n",
    "            'ab':  {'': 0, 'a': 0},\n",
    "            'ba':  {'': 0, 'a': 0},\n",
    "            'bb':  {'': 0, 'a': 0},\n",
    "            'baa': {'': 0, 'a': 0},\n",
    "            'bab': {'': 0, 'a': 0}}\n",
    "    o.P, o.S = P, S\n",
    "    g, s = o.grammar()\n",
    "    print('start: ', s)\n",
    "    for k in g:\n",
    "        print(k)\n",
    "        for r in g[k]:\n",
    "            print(\" | \", r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are convinced that we can produce a DFA or a grammar out of the table let us proceed to examining how to produce this table.\n",
    "\n",
    "We start with the start state in the table, because we know for sure that it exists, and is represented by the empty string in row and column,\n",
    "which together (prefix + suffix) is the empty string `''` or $ \\epsilon $.  We ask the program if it accepts the empty string, and if it accepts, we mark  the corresponding cell in the table as *accept* (or `1`).\n",
    "\n",
    "For any given state in the DFA, we should be able to say what happens when an input symbol is fed into the machine in that state. So, we can extend the    table with what happens when each input symbol is fed into the start state.  This means that we extend the table with rows corresponding to each symbol in the input alphabet.\n",
    "\n",
    "So, we can initialize the table as follows. First, we check whether the empty string is in the language. Then, we extend the table `T` to `(P u P.A).S`    using membership queries. This is given in `update_table()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTable(ObservationTable):\n",
    "    def init_table(self, oracle):\n",
    "        self._T[''] = {'': oracle.is_member('') }\n",
    "        self.update_table(oracle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update table has two parts. First, it takes the current set of prefixes (`rows`) and determines the auxiliary rows to compute based on extensions of   the current rows with the symbols in the alphabet (`auxrows`). This gives the complete set of rows for the table. Then, for each suffix in `S`, ensure     that the table has a cell, and it is updated with the oracle result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTable(ObservationTable):\n",
    "    def update_table(self, oracle):\n",
    "        def unique(l): return list({s:None for s in l}.keys())\n",
    "        rows = self.P\n",
    "        auxrows = [p + a for p in self.P for a in self.A]\n",
    "        PuPxA = unique(rows + auxrows)\n",
    "        for p in PuPxA:\n",
    "            if p not in self._T: self._T[p] = {}\n",
    "            for s in self.S:\n",
    "                if p in self._T and s in self._T[p]: continue\n",
    "                self._T[p][s] = oracle.is_member(p + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    o = ObservationTable(alphabet)\n",
    "    def orcl(): pass\n",
    "    orcl.is_member = lambda x: 1\n",
    "    o.init_table(orcl)\n",
    "    for p in o._T: print(p, o._T[p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While doing this, there is one requirement we need to ensure. The result of transition from every state for every alphabet needs to be defined.  The property that ensures this for the observation table is called *closedness* or equivalently, the observation table is *closed* if the table has the following property.\n",
    "\n",
    "### Closed\n",
    "The idea is that for every prefix we have, in set $ P $, we need to find the state that is reached for every $ a \\in A $. Then, we need to make sure that the *state* represented by that   prefix exists in $ P $. (If such a state does not exist in P, then it means that we have found a new state).\n",
    "\n",
    "Formally: An observation table $ P \\times S $ is closed if for each $ t \\in P·A $ there exists a $ p \\in P $ such that $ [t] = [p] $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTable(ObservationTable):\n",
    "    def closed(self):\n",
    "        states_in_P = {self.state(p) for p in self.P}\n",
    "        P_A = [p+a for p in self.P for a in self.A]\n",
    "        for t in P_A:\n",
    "            if self.state(t) not in states_in_P: return False, t\n",
    "        return True, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    def orcl(): pass\n",
    "    orcl.is_member = lambda x: 1 if x in ['a'] else 0\n",
    "\n",
    "    ot = ObservationTable(list('ab'))\n",
    "    ot.init_table(orcl)\n",
    "    for p in ot._T: print(p, ot._T[p])\n",
    "\n",
    "    res, counter = ot.closed()\n",
    "    assert not res\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTable(ObservationTable):\n",
    "    def add_prefix(self, p, oracle):\n",
    "        if p in self.P: return\n",
    "        self.P.append(p)\n",
    "        self.update_table(oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    def orcl(): pass\n",
    "    orcl.is_member = lambda x: 1 if x in ['a'] else 0\n",
    "\n",
    "    ot = ObservationTable(list('ab'))\n",
    "    ot.init_table(orcl)\n",
    "    res, counter = ot.closed()\n",
    "    assert not res\n",
    "\n",
    "    ot.add_prefix('a', orcl)\n",
    "    for p in ot._T: print(p, ot._T[p])\n",
    "    res, counter = ot.closed()\n",
    "    assert res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially the intuition behind most of the grammar inference algorithms, and the cleverness lies in how the suffixes are chosen. In the case of L\\*, when we find that one of the transitions from the current states result in a new state, we add the alphabet that caused the transition from the current state and the suffix that distinguished the new state to the     suffixes (i.e, a + suffix is added to the columns).\n",
    "\n",
    "This particular aspect is governed by the *consistence* property of the observation table.\n",
    "\n",
    "### Consistent\n",
    "\n",
    "An observation table $ P \\times S $ is consistent if, whenever $ p1 $ and $ p2 $ are elements of P such that $ [p1] = [p2] $, for each $ a \\in A $, $ [p1.a] = [p2.a] $.  *If* there are    two rows in the top part of the table repeated, then the corresponding suffix results should be the same.  If not, we have found a counter example. So we report the alphabet and the       suffix that distinguished the rows. We will then add the new string (a + suffix) as a new suffix to the table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTable(ObservationTable):\n",
    "    def consistent(self):\n",
    "        matchingpairs = [(p1, p2) for p1 in self.P for p2 in self.P\n",
    "                         if p1 != p2 and self.state(p1) == self.state(p2)]\n",
    "        suffixext = [(a, s) for a in self.A for s in self.S]\n",
    "        for p1,p2 in matchingpairs:\n",
    "            for a, s in suffixext:\n",
    "                if self.cell(p1+a,s) != self.cell(p2+a,s):\n",
    "                        return False, (p1, p2), (a + s)\n",
    "        return True, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObservationTable(ObservationTable):\n",
    "    def add_suffix(self, a_s, oracle):\n",
    "        if a_s in self.S: return\n",
    "        self.S.append(a_s)\n",
    "        self.update_table(oracle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    def orcl(): pass\n",
    "    orcl.is_member = lambda x: 1 if x in ['a'] else 0\n",
    "\n",
    "    ot = ObservationTable(list('ab'))\n",
    "    ot.init_table(orcl)\n",
    "    is_closed, counter = ot.closed()\n",
    "    assert not is_closed\n",
    "    ot.add_prefix('a', orcl)\n",
    "    ot.add_prefix('b', orcl)\n",
    "    ot.add_prefix('ba', orcl)\n",
    "    for p in ot._T: print(p, ot._T[p])\n",
    "\n",
    "    is_closed, unknown_P = ot.closed() \n",
    "    print(is_closed)\n",
    "\n",
    "    is_consistent,_, unknown_A = ot.consistent() \n",
    "    assert not is_consistent\n",
    "\n",
    "    ot.add_suffix('a', orcl)\n",
    "    for p in ot._T: print(p, ot._T[p])\n",
    "\n",
    "    is_consistent,_, unknown_A = ot.consistent() \n",
    "    assert is_consistent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Of course readers will quickly note that the table is not the best data structure here, and just because a suffix distinguished two particular states does not mean that it is a good idea to evaluate the same suffix on all other states. These are ideas that will be explored in later algorithms).\n",
    "\n",
    "\n",
    "Finally, L\\* also relies on a *Teacher* for it to suggest new suffixes that can distinguish unrecognized states from current ones.\n",
    "\n",
    "## Teacher\n",
    "\n",
    "We now construct our teacher. We have two requirements for the teacher.  The first is that it should fulfil the requirement for Oracle. That is, it should answer `is_member()` queries.    Secondly, it should also answer `is_equivalent()` queries.\n",
    "\n",
    "First, we define the oracle interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Oracle:\n",
    "    def is_member(self, q): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple teacher based on regular grammars. That is, if you give it a regular grammar, will convert it to an acceptor based on a parser and a generator based on a grammar fuzzer, and will then use it for verification of hypothesis grammars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PAC Learning\n",
    "\n",
    "PAC learning was introduced by Valiant in 1984 as a way to think about inferred models in computational linguistics and machine learning.  The basic idea is that given a    blackbox model, we need to be able to produce samples that can then be tested against the model to construct an inferred model (i.e, to train the model). For sampling during training, we  have to assume some sampling procedure, and hence a distribution for training.  Per PAC learning, we can only guarantee the performance of the learned model when tested using samples from the same distribution. Given that we are sampling from a distribution, there is a possibility that due to non-determinism, the data is not as spread out as we may like, and hence the      training data is not optimal by a certain probability. This reflects on the quality of the model learned. This is indicated by the concept of confidence intervals, and indicated by the $  \\delta $ parameter. That is, $ 1 - \\delta $ quantifies the confidence we have in our model. Next, given any training data, due to the fact that the training data is finite, our grammar    learned is an approximation of the real grammar, and there will always be an error term. This error is quantified by the $ \\epsilon $ parameter. Given the desired $ \\delta $ and $         \\epsilon $ Angluin provides a formula to compute the number of calls to make to the membership oracle at the $ i^{th} $ equivalence query.\n",
    "\n",
    "$$ n=\\lceil\\frac{1}{\\epsilon}\\times log(\\frac{1}{\\delta}+i\\times log(2))\\rceil $$\n",
    "\n",
    "In essence the PAC framework says that there is $ 1 - \\delta  $ probability that the model learned will be approximately correct. That is, it will classify samples with an error rate less than $ \\epsilon $.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(Oracle):\n",
    "    def is_equivalent(self, grammar, start): assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input the PAC parameters delta for confidence and epsilon for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(Teacher):\n",
    "    def __init__(self, re_g, re_s, delta=0.5, epsilon=0.5):\n",
    "        self.g, self.s = re_g, re_s\n",
    "        self.parser = parser.EarleyParser(self.g)\n",
    "        self.sampler = fuzzer.LimitFuzzer(self.g)\n",
    "        self.equivalence_query_counter = 0\n",
    "        self.delta, self.epsilon = delta, epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define the membership query `is_member()` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(Teacher):\n",
    "    def is_member(self, q):\n",
    "        try: list(self.parser.recognize_on(q, self.s))\n",
    "        except: return 0\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a grammar, check whether it is equivalent to the given grammar.\n",
    "The PAC guarantee is that we only need `num_calls` for the `i`th equivalence\n",
    "query. For equivalence check here, we check for strings of length 1, then\n",
    "length 2 etc, whose sum should be `num_calls`. We take the easy way out here,\n",
    "and just use `num_calls` as the number of calls for each string length.\n",
    "We have what is called a *cooperative teacher*, that tries to respond with\n",
    "a shortest possible counter example. We # also take the easy way out and only\n",
    "check for a maximum length of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(Teacher):\n",
    "    def is_equivalent(self, grammar, start, max_length_limit=10):\n",
    "        self.equivalence_query_counter += 1\n",
    "        num_calls = math.ceil(1.0/self.epsilon *\n",
    "                  (math.log(1.0/self.delta) +\n",
    "                              self.equivalence_query_counter * math.log(2)))\n",
    "\n",
    "        for limit in range(1, max_length_limit):\n",
    "            is_eq, counterex, c = self.is_equivalent_for(self.g, self.s,\n",
    "                                                    grammar, start, num_calls)\n",
    "            if counterex is None: # no members of length limit\n",
    "                continue\n",
    "            if not is_eq:\n",
    "                c = [a for a in counterex if a is not None][0]\n",
    "                return False, c\n",
    "        return True, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to remove epsilon tokens from places other than the start rule to make the grammar well behaved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(Teacher):\n",
    "    def digest_grammar(self, g, s):\n",
    "        if not g[s]: return 0, None, None\n",
    "        rgf = fuzzer.LimitFuzzer(g)\n",
    "        ep = parser.EarleyParser(g)\n",
    "        return ep, rgf\n",
    "\n",
    "    def gen_random(self, f, s):\n",
    "        return f.fuzz(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Grammar Equivalence\n",
    "Checking if two grammars are equivalent to a length of string for n count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Teacher(Teacher):\n",
    "    def is_equivalent_for(self, g1, s1, g2, s2, n):\n",
    "        ep1, lf1 = self.digest_grammar(g1, s1)\n",
    "        ep2, lf2 = self.digest_grammar(g2, s2)\n",
    "        count = 0\n",
    "\n",
    "        str1 = {self.gen_random(lf1, s1) for _ in range(n)}\n",
    "        str2 = {self.gen_random(lf2, s2) for _ in range(n)}\n",
    "\n",
    "        for st1 in str1:\n",
    "            if st1 is None: continue\n",
    "            count += 1\n",
    "            try:\n",
    "                #print('st1', st1, ' recognize on s2:', s2)\n",
    "                list(ep2.recognize_on(st1, s2))\n",
    "            except Exception as e:\n",
    "                return False, (st1, None), count\n",
    "\n",
    "        for st2 in str2:\n",
    "            if st2 is None: continue\n",
    "            count += 1\n",
    "            try:\n",
    "                #print('st2', st1, ' recognize on s1:', s1)\n",
    "                list(ep1.recognize_on(st2, s1))\n",
    "            except Exception as e:\n",
    "                return False, (None, st2), count\n",
    "\n",
    "        return True, None, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    g1 = { # should end with one.\n",
    "            '<0>': [\n",
    "                ['1', '<1>'],\n",
    "                ['0', '<0>']\n",
    "                ],\n",
    "            '<1>':[\n",
    "                ['1', '<1>'],\n",
    "                []\n",
    "                ]\n",
    "    }\n",
    "    g2 = { # should end with one.\n",
    "            '<0>': [\n",
    "                ['1', '<1>'],\n",
    "                ['0', '<1>'] # changed here.\n",
    "                ],\n",
    "            '<1>':[\n",
    "                ['1', '<1>'],\n",
    "                []\n",
    "                ]\n",
    "    }\n",
    "    s1 = '<0>'\n",
    "    s2 = '<0>'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    t = Teacher(g1, '<0>')\n",
    "    parse1, fuzz1 = t.digest_grammar(g1, s1)\n",
    "    parse2, fuzz2 = t.digest_grammar(g2, s2)\n",
    "    #for v in parse2.parse_on('000111', s1):\n",
    "    #    print(utils.tree_to_str(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    v = t.is_equivalent_for(g1, '<0>', g2, '<0>', 10)\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_str = fuzz2.fuzz(s2)\n",
    "print(my_str)\n",
    "for v in parse2.parse_on(my_str, s2):\n",
    "    print(utils.tree_to_str(v))\n",
    "    utils.display_tree(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L star main loop\n",
    "\n",
    "Given the observation table and the teacher, the algorithm itself is simple. The L* algorithm loops, doing the following operations in sequence. (1) keep the table closed, (2) keep the table consistent, and if it is closed and consistent (3) ask the teacher if the corresponding hypothesis grammar is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l_star(T, teacher):\n",
    "    T.init_table(teacher)\n",
    "\n",
    "    while True:\n",
    "        print('+')\n",
    "        while True:\n",
    "            print('.', end='', flush=True)\n",
    "            is_closed, unknown_P = T.closed()\n",
    "            is_consistent, _, unknown_AS = T.consistent()\n",
    "            if is_closed and is_consistent: break\n",
    "            if not is_closed: T.add_prefix(unknown_P, teacher)\n",
    "            if not is_consistent: T.add_suffix(unknown_AS, teacher)\n",
    "\n",
    "        grammar, start = T.grammar()\n",
    "        eq, counterX = teacher.is_equivalent(grammar, start)\n",
    "        if eq: return grammar, start\n",
    "        print('Equivalence query: counter', counterX)\n",
    "        for i,_ in enumerate(counterX): T.add_prefix(counterX[0:i+1], teacher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import string\n",
    "    e = '(ab|cd|ef)*'\n",
    "    eg = {\n",
    "          '<X>': [['a', 'b'], ['c', 'd'], ['e', 'f']],\n",
    "          '<Xs>': [['<X>', '<Xs>'], []],\n",
    "          '<start>': [['<Xs>']],\n",
    "    }\n",
    "\n",
    "    es = '<start>'\n",
    "    teacher = Teacher(eg,es)\n",
    "    tbl = ObservationTable(list(string.ascii_letters))\n",
    "    g, s = l_star(tbl, teacher)\n",
    "\n",
    "    gf = fuzzer.LimitFuzzer(g)\n",
    "    for i in range(10):\n",
    "        res = gf.fuzz(key=s, max_depth=100)\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzingbook.GrammarMiner as miner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miner.syntax_diagram(utils.to_fuzzingbook_grammar(g))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
