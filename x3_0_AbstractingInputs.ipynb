{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstracting Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_4_HierarchicalReducer as hdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_2_GrammarFuzzer as fuzzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_3_EarleyParser as parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.x0_4_HierarchicalReducer import PRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_1_Grammars as grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def expr_double_paren(inp):\n",
    "    if re.match(r'.*[(][(].*[)][)].*', inp):\n",
    "        return PRes.success\n",
    "    return PRes.failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    expr_parser = parser.EarleyParser(grammars.EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    my_input = '1+((2*3/4))'\n",
    "    assert expr_double_paren(my_input) == PRes.success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parsed_expr = list(expr_parser.parse_on(my_input, grammars.EXPR_START))[0]\n",
    "    utils.display_tree(parsed_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    reduced_expr_tree = hdd.hierarchical_reduction(parsed_expr, grammars.EXPR_GRAMMAR, expr_double_paren)\n",
    "    utils.display_tree(reduced_expr_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDSet Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddset(reduced_tree, grammar, predicate):\n",
    "    vals = generalize(reduced_tree, [], [], grammar, predicate)\n",
    "    ta = get_abstract_tree(reduced_tree, vals)\n",
    "    return ta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `generalize()` procedure tries to generalize a given tree recursively. For that, it starts at the root node, and replaces the node with\n",
    "a randomly generated tree rooted at the same node. It tries that a configurable number of times, and if the tree can be replaced each time\n",
    "without failure, then we mark the path as abstract. If not, we descent into its children and try the same. While generating a new tree, any\n",
    "previous nodes marked as abstract is also replaced by randomly generated values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_topdown(tree, path, known_paths, grammar, predicate):\n",
    "    node = hdd.get_child(tree, path)\n",
    "    if not utils.is_nt(node[0]): return known_paths\n",
    "    if can_abstract(tree, path, known_paths, grammar, predicate):\n",
    "        known_paths.append(path)\n",
    "        return known_paths\n",
    "    for i,child in enumerate(node[1]):\n",
    "        ps = generalize(tree, path + [i], known_paths, grammar, predicate)\n",
    "    return known_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_bottomup(tree, path, known_paths, grammar, predicate):\n",
    "    node = hdd.get_child(tree, path)\n",
    "    if not utils.is_nt(node[0]): return []\n",
    "    kpaths = []\n",
    "    for i,child in enumerate(node[1]):\n",
    "        ps = generalize(tree, path + [i], kpaths + known_paths, grammar, predicate)\n",
    "        if ps:\n",
    "            kpaths.extend(ps)\n",
    "        \n",
    "    if can_abstract(tree, path, known_paths, grammar, predicate):\n",
    "        return [path]\n",
    "    return kpaths + known_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize(tree, path, known_paths, grammar, predicate):\n",
    "    return generalize_bottomup(tree, path, known_paths, grammar, predicate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `can_abstract()` procedure above does the checking to see if the tree can be abstracted. It is implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRIES_FOR_ABSTRACTION = 100\n",
    "\n",
    "def can_abstract(tree, path, known_paths, grammar, predicate):\n",
    "    i = 0\n",
    "    while (i < MAX_TRIES_FOR_ABSTRACTION):\n",
    "        t = replace_all_paths_with_generated_values(tree, known_paths + [path], grammar)\n",
    "        s = utils.tree_to_str(t)\n",
    "        if predicate(s) == hdd.PRes.failed:\n",
    "            return False\n",
    "        elif predicate(s) == hdd.PRes.invalid:\n",
    "            continue\n",
    "        i += 1\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `can_abstract()` procedure tries to generate a valid value `MAX_TRIES_FOR_ABSTRACTION` times. For this, it relies on\n",
    "`replace_all_paths_with_generated_values()` which is implemented as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_all_paths_with_generated_values(tree, paths, grammar):\n",
    "    my_tree = tree\n",
    "    for p in paths:\n",
    "        my_tree = replace_path_with_generated_value(my_tree, p, grammar)\n",
    "    return my_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the major work is done by `replace_path_with_generated_value()` which replaces a single given path with a generated node\n",
    "of the same kind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_path(tree, path, new_node=None):\n",
    "    if new_node is None: new_node = []\n",
    "    if not path: return utils.deep_copy(new_node)\n",
    "    cur, *path = path\n",
    "    name, children, *rest = tree\n",
    "    new_children = []\n",
    "    for i,c in enumerate(children):\n",
    "        if i == cur:\n",
    "            nc = replace_path(c, path, new_node)\n",
    "        else:\n",
    "            nc = c\n",
    "        if nc:\n",
    "            new_children.append(nc)\n",
    "    return (name, new_children, *rest)\n",
    "\n",
    "def replace_path_with_generated_value(tree, path, grammar):\n",
    "    node = hdd.get_child(tree, path)\n",
    "    s, gnode = generate_random_value(grammar, node[0])\n",
    "    t = replace_path(tree, path, gnode)\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a key, generate a random value for that key using the grammar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_value(grammar, key):\n",
    "    my_fuzzer = fuzzer.LimitFuzzer(grammar)\n",
    "    node = my_fuzzer.iter_gen_key(key, max_depth=10)\n",
    "    return (utils.tree_to_str(node), node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the converter from an abstract tree to a string expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_tree_to_str(tree):\n",
    "    name, children, *general_ = tree\n",
    "    if not utils.is_nt(name): return name\n",
    "    if is_node_abstract(tree):\n",
    "        return name\n",
    "    return ''.join([abstract_tree_to_str(c) for c in children])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a few library functions for marking some nodes concrete and some abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_concrete_r(tree):\n",
    "    name, children, *abstract_a = tree\n",
    "    abstract = {'abstract': False} if not abstract_a else abstract_a[0]\n",
    "    return (name, [mark_concrete_r(c) for c in children], abstract)\n",
    "\n",
    "def mark_path_abstract(tree, path):\n",
    "    name, children = hdd.get_child(tree, path)\n",
    "    new_tree = replace_path(tree, path, (name, children, {'abstract': True}))\n",
    "    return new_tree\n",
    "\n",
    "def get_abstract_tree(tree, paths):\n",
    "    for path in paths:\n",
    "        tree = mark_path_abstract(tree, path)\n",
    "    return mark_concrete_r(tree)\n",
    "\n",
    "def is_node_abstract(node):\n",
    "    name, children, *abstract_a = node\n",
    "    if not abstract_a:\n",
    "        return True\n",
    "    else:\n",
    "        return abstract_a[0]['abstract']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we are ready to extract our pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DisplayAbstractTree(utils.DisplayTree):\n",
    "    def extract_node(self, node, id):                                                      \n",
    "        symbol, children, *annotation = node \n",
    "        if is_node_abstract(node): return symbol, [], ''\n",
    "        return symbol, children, ''.join(str(a) for a in annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_abstract_tree(node):\n",
    "    return DisplayAbstractTree(node).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pattern = ddset(reduced_expr_tree, grammars.EXPR_GRAMMAR, expr_double_paren)\n",
    "    display_abstract_tree(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_str_abs(node):\n",
    "    symbol, children, *rest = node\n",
    "    if is_node_abstract(node): return symbol\n",
    "    if utils.is_nt(symbol):\n",
    "        return ''.join([tree_to_str_abs(c) for c in children])\n",
    "    else:\n",
    "        return symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(tree_to_str_abs(pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Fault Grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have an abstract input. However, this input only abstracts _inner holes_. That is, it does not abstract the context in which this string can be inserted and still produce a failure. For example, an input string `((1))+1` can produce the same failure. Hence, we need to abstract the context. This is what we will tackle next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic idea is that if we can find an abstract subtree of the evocative pattern derivation tree, such that the presence of this abstract subtree in the input guarantees the failure, then we can modify the grammar such that this abstract subtree is always present (we call the abstract subtree an evocative subtree). That is, for any input from such a grammar, at least one instance of the evocative subtree will be present. This is fairly easy to do if the generated tree contains a nonterminal of the same kind as that of the top node of the evocative subtree. Simply replace that node with the evocative subtree, and fill in the abstract nonterminals in the evocative subtree with concrete expansions.\n",
    "\n",
    "So, what we are going to do, is to start from the top of the tree and for each node ordered by the depth, check each node to see whether the subtree rooted on that node is sufficient for inducing the fault if it is present in a larger tree. For this, we need to construct a grammar that can guarantee that a particular subtree is present in all inputs generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reachable Grammar\n",
    "\n",
    "What we want to do is to modify the grammar such that at least one instance of such a nonterminal is present. Such a grammar is called the `reachable_grammar`. To produce a reaching grammar, first we need to find what nonterminals are reachable from the expansion of a given nonterminal. A nonterminal `<A>` is reachable from another nonterminal `<B>` if and only if one of the expansion rules for `<B>` contains the nonterminal `<A>` or it is reachable from one of the nonterminals in the expansion rules of `<B>`. Note that it is not enough to be the same nonterminal. That is, (for e.g.) `<A>` is not # reachable from `<A>` if expansion rules of `<A>` contain only terminal symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_reachable_keys(grammar, key, reachable_keys=None, found_so_far=None):\n",
    "    if reachable_keys is None: reachable_keys = {}\n",
    "    if found_so_far is None: found_so_far = set()\n",
    "\n",
    "    for rule in grammar[key]:\n",
    "        for token in rule:\n",
    "            if not utils.is_nt(token): continue\n",
    "            if token in found_so_far: continue\n",
    "            found_so_far.add(token)\n",
    "            if token in reachable_keys:\n",
    "                for k in reachable_keys[token]:\n",
    "                    found_so_far.add(k)\n",
    "            else:\n",
    "                keys = find_reachable_keys(grammar,\n",
    "                        token, reachable_keys, found_so_far)\n",
    "    return found_so_far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for key in grammars.EXPR_GRAMMAR:\n",
    "        keys = find_reachable_keys(grammars.EXPR_GRAMMAR, key, {})\n",
    "        print(key, keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now collect it in a data structure for easier access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachable_dict(grammar):\n",
    "    reachable = {}\n",
    "    for key in grammar:\n",
    "        keys = find_reachable_keys(grammar, key, reachable)\n",
    "        reachable[key] = keys\n",
    "    return reachable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, if we want to know the nonterminals that are reachable from `<integer>`, we can simply do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    reaching = reachable_dict(grammars.EXPR_GRAMMAR)\n",
    "    print(reaching['<integer>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is, only `<digit>` and `<integer>` are reachable from the expansion of nonterminal `<integer>`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reachable positions.\n",
    "\n",
    "Next, given an evocative subtree, we want to find what tokens of the grammar can actually embed such a subtree. We call the top node of an evocative subtree its characterizing node, and the nonterminal the characterizing nonterminal of the evocative subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reachable_positions(rule, fkey, reachable):\n",
    "    positions = []\n",
    "    for i, token in enumerate(rule):\n",
    "        if not utils.is_nt(token): continue\n",
    "        if fkey == token or fkey in reachable[token]:\n",
    "            positions.append(i)\n",
    "    return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we assume that `<factor>` is the characterizing nonterminal. Here are the locations in grammar where one can embed the evocative subtree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for k in grammars.EXPR_GRAMMAR:\n",
    "        print(k)\n",
    "        for rule in grammars.EXPR_GRAMMAR[k]:\n",
    "            v = get_reachable_positions(rule, '<factor>', reaching)\n",
    "            print('\\t', rule, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insertion Grammar\n",
    "\n",
    "Given the insertion locations, can we produce a grammar such that we can guarantee that at least one instance of evocative subtree can be inserted? To do that, all we need to guarantee is that the start node in any derivation tree can reach the characterizing nonterminal. For now, let us call our fault `F1`. Let us indicate that our start symbol guarantees reachability of characterizing nonterminal by specializing it. So, our new start symbol is `<start F1>`\n",
    "\n",
    "Next, for the start symbol to guarantee reachability to characterizing nonterminal, all that we need to ensure is that all the expansion rules of start can reach the characterizing nonterminal. On the other hand, for a guarantee that an expansion rule can reach the characterizing nonterminal, all that is required is that one of the nonterminals in that rule guarantees reachability. We start with a few helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplit(token):\n",
    "    assert token[0], token[-1] == ('<', '>')\n",
    "    front, *back = token[1:-1].split(None, 1)\n",
    "    return front, ' '.join(back)\n",
    "\n",
    "def refinement(token):\n",
    "    return tsplit(token)[1].strip()\n",
    "\n",
    "def is_refined_key(key):\n",
    "    assert utils.is_nt(key)\n",
    "    return (' ' in key)\n",
    "\n",
    "def is_base_key(key):\n",
    "    return not is_refined_key(key)\n",
    "\n",
    "def stem(token):\n",
    "    return tsplit(token)[0].strip()\n",
    "\n",
    "def normalize(token):\n",
    "    assert utils.is_nt(token), token\n",
    "    if is_base_key(token): return token\n",
    "    return '<%s>' % stem(token)\n",
    "\n",
    "def refine_base_key(k, prefix):\n",
    "    assert utils.is_nt(k), k\n",
    "    assert is_base_key(k), k\n",
    "    return '<%s %s>' % (stem(k), prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the `reachable_key()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachable_key(grammar, key, cnodesym, suffix, reachable):\n",
    "    rules = grammar[key]\n",
    "    my_rules = []\n",
    "    for rule in grammar[key]:\n",
    "        positions = get_reachable_positions(rule, cnodesym, reachable)\n",
    "        if not positions:\n",
    "            # skip this rule because we can not embed the fault here.\n",
    "            continue\n",
    "        else:\n",
    "            # at each position, insert the cnodesym\n",
    "            for pos in positions:\n",
    "                new_rule = [refine_base_key(t, suffix)\n",
    "                            if pos == p else t for p,t in enumerate(rule)]\n",
    "                my_rules.append(new_rule)\n",
    "    return (refine_base_key(key, suffix), my_rules)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for key in grammars.EXPR_GRAMMAR:\n",
    "        fk, rules = reachable_key(grammars.EXPR_GRAMMAR, key, '<factor>', 'F1', reaching)\n",
    "        print(fk)\n",
    "        for r in rules:\n",
    "            print('    ', r)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Grammar\n",
    "\n",
    "Next, we need to ensure that our evocative subtree can form a unique subtree. For that, all we need to do is that all nodes in the subtree are named uniquely. Not all nodes in the evocative subtree needs unique names however. DDSet produces trees such that some nodes in the tree are left abstract. We leave these with the original node names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_unique_name(symbol, suffix, i):\n",
    "    return '<%s %s_%s>' % (symbol[1:-1], suffix, str(i))\n",
    "\n",
    "def mark_unique_nodes(node, suffix, counter=None):\n",
    "    if counter is None: counter = [0]\n",
    "    symbol, children, *abstract = node\n",
    "    if is_node_abstract(node): # we don't markup further\n",
    "        return node\n",
    "    if utils.is_nt(symbol):\n",
    "        i = counter[0]\n",
    "        counter[0] += 1\n",
    "        cs = [mark_unique_nodes(c, suffix, counter) for c in children]\n",
    "        return (mark_unique_name(symbol, suffix, i), cs, *abstract)\n",
    "    else:\n",
    "        assert not children\n",
    "        return (symbol, children, *abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    evocative_pattern = pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    unique_pattern_tree = mark_unique_nodes(evocative_pattern, 'F1')\n",
    "    display_abstract_tree(unique_pattern_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_cnode_to_grammar(tree, grammar=None):\n",
    "    if grammar is None: grammar = {}\n",
    "    if is_node_abstract(tree): return grammar, normalize(tree[0])\n",
    "    name, children, *rest = tree\n",
    "    tokens = []\n",
    "    if name not in grammar: grammar[name] = []\n",
    "    for c in children:\n",
    "        n, cs, *rest = c\n",
    "        tokens.append(n)\n",
    "        if utils.is_nt(n):\n",
    "            unique_cnode_to_grammar(c, grammar)\n",
    "    grammar[name].append(tokens)\n",
    "    return grammar, tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    g,s = unique_cnode_to_grammar(unique_pattern_tree)\n",
    "    utils.display_grammar(g,s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define `pattern_grammar()` that wraps both calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_grammar(cnode, fname):\n",
    "    unique_pattern_tree = mark_unique_nodes(cnode, fname)\n",
    "    pattern_g, pattern_s = unique_cnode_to_grammar(unique_pattern_tree)\n",
    "    return pattern_g, pattern_s, unique_pattern_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pattern_g,pattern_s, t = pattern_grammar(evocative_pattern, 'F1')\n",
    "    utils.display_grammar(pattern_g, pattern_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a procedure to reverse our pattern grammar to ensure we can get back our tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pattern_grammar_to_tree(g, s):\n",
    "    if is_base_key(s):\n",
    "        return (s, [])\n",
    "    rules = g[s]\n",
    "    assert len(rules) == 1\n",
    "    children = [pattern_grammar_to_tree(g,t) if t in g else (t, [])\n",
    "            for t in rules[0]]\n",
    "    return (s, children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    c_tree = pattern_grammar_to_tree(pattern_g,pattern_s)\n",
    "    utils.display_tree(c_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the reaching grammar and the pattern grammar, we can combine them to produce the complete grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reachable_grammar(grammar, start, cnodesym, suffix, reachable):\n",
    "    new_grammar = {}\n",
    "    s_key = None\n",
    "    for key in grammar:\n",
    "        fk, rules = reachable_key(grammar, key, cnodesym, suffix, reachable)\n",
    "        assert fk not in new_grammar\n",
    "        if key == start: s_key = fk\n",
    "        new_grammar[fk] = rules\n",
    "    return new_grammar, s_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    evocative_subtree = evocative_pattern[1][0][1][0][1][0]\n",
    "    my_key_f = evocative_subtree[0]\n",
    "    reaching = reachable_dict(grammars.EXPR_GRAMMAR)\n",
    "    reach_g, reach_s = reachable_grammar(grammars.EXPR_GRAMMAR,\n",
    "            grammars.EXPR_START, my_key_f, 'F1', reaching)\n",
    "    utils.display_grammar(reach_g, reach_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert A Fault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def atleast_one_fault_grammar(grammar, start_symbol, cnode, fname):\n",
    "    key_f = cnode[0]\n",
    "    pattern_g, pattern_s, t = pattern_grammar(cnode, fname)\n",
    "\n",
    "    reachable_keys = reachable_dict(grammar)\n",
    "    reach_g, reach_s = reachable_grammar(grammar,\n",
    "            start_symbol, key_f, fname, reachable_keys)\n",
    "\n",
    "    combined_grammar = {**grammar, **pattern_g, **reach_g}\n",
    "    reaching_sym = refine_base_key(key_f, fname)\n",
    "    combined_grammar[reaching_sym] = reach_g[reaching_sym] + pattern_g[pattern_s]\n",
    "\n",
    "    return combined_grammar, reach_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    g, s = utils.grammar_gc(*atleast_one_fault_grammar(grammars.EXPR_GRAMMAR,\n",
    "        grammars.EXPR_START, evocative_subtree, 'F1'))\n",
    "    utils.display_grammar(g, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number the nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we are ready to check whether any node in our abstract input is suffient to induce the given fault. We first uniquely number the nodes in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numbered_nodes(node, count, keys):\n",
    "    symbol, children, *rest = node\n",
    "    if is_node_abstract(node):\n",
    "        return symbol, children, *rest\n",
    "    if utils.is_nt(symbol):\n",
    "        key = \"<%s %d>\" % (symbol[1:-1], count[0])\n",
    "        keys.append((key, node))\n",
    "        count[0] += 1\n",
    "        return key, [numbered_nodes(c, count, keys) for c in children], *rest\n",
    "    else:\n",
    "        return symbol, children, *rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    keys = []\n",
    "    nnodes = numbered_nodes(pattern, [0], keys)\n",
    "    display_abstract_tree(nnodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for k,v in keys:\n",
    "        print(k, v[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_tree_to_grammar(node, grammar = None):\n",
    "    if grammar is None: grammar = {}\n",
    "    symbol, children, *rest = node\n",
    "    grammar[symbol] = [[c[0] for c in children]]\n",
    "    if is_node_abstract(node):\n",
    "        return grammar\n",
    "    if utils.is_nt(symbol):\n",
    "        [abs_tree_to_grammar(c, grammar) for c in children]\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(abs_tree_to_grammar(nnodes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstract_context(base_grammar,base_key, tree, predicate):\n",
    "    keys = []\n",
    "    nnodes = numbered_nodes(tree, [0], keys)\n",
    "    for k in reversed(keys):\n",
    "        abs_key, node = k\n",
    "        # replace the base_grammar key by abstract grammar and check if it works.\n",
    "        # if it works, that is our grammar.\n",
    "        new_g,new_s = atleast_one_fault_grammar(base_grammar, base_key, node, 'f1')\n",
    "        if is_valid_grammar(new_g, new_s, predicate):\n",
    "            return new_s, new_g\n",
    "    return base_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_grammar(g, s, predicate):\n",
    "    for i in range(10):\n",
    "        v,node = generate_random_value(g, s)\n",
    "        if predicate(v) == PRes.failed:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    new_s, new_g = abstract_context(grammars.EXPR_GRAMMAR, grammars.EXPR_START, pattern, expr_double_paren)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    utils.display_grammar(new_g, new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_to_str_abs(node):\n",
    "    symbol, children, *rest = node\n",
    "    if utils.is_nt(symbol):\n",
    "        if is_base_key(symbol): return symbol\n",
    "        return ''.join([tree_to_str_abs(c) for c in children])\n",
    "    else:\n",
    "        return symbol\n",
    "\n",
    "def get_faults(tree):\n",
    "    sym, children = tree\n",
    "    res = []\n",
    "    if sym == '<fault>':\n",
    "        return [utils.tree_to_str(tree)]\n",
    "    for c in children:\n",
    "        v = get_faults(c)\n",
    "        res.extend(v)\n",
    "    return res\n",
    "\n",
    "def get_pattern_grammar(g, fault):\n",
    "    for k in g:\n",
    "        if fault != refinement(k): continue\n",
    "        for rule in g[k]:\n",
    "            for t in rule:\n",
    "                if not utils.is_nt(t): continue\n",
    "                if is_base_key(t): continue\n",
    "                if refinement(t) != fault:\n",
    "                    g_ = utils.deep_copy(g)\n",
    "                    g_[k] = [rule]\n",
    "                    return g_, k\n",
    "    return None, None\n",
    "            \n",
    "\n",
    "def get_fault_summary(g, fault):\n",
    "    pg_, ps = get_pattern_grammar(g, fault)\n",
    "    pg, ps = utils.grammar_gc(pg_, ps)\n",
    "    t = pattern_grammar_to_tree(pg, ps)\n",
    "    v = tree_to_str_abs(t)\n",
    "    return ps, v\n",
    "\n",
    "def summarize_fault_grammar(g, s):\n",
    "    eparser = parser.EarleyParser(grammars.BEXPR_GRAMMAR)\n",
    "    ex = refinement(s)\n",
    "    tree = list(eparser.parse_on(ex, grammars.BEXPR_START))[0]\n",
    "    print(s + \"\\n     where  \" + \"\\n            \".join([\"%s is %s\" % get_fault_summary(g, f) for f in get_faults(tree)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    summarize_fault_grammar(new_g, new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    new_f = fuzzer.LimitFuzzer(new_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    for i in range(10):\n",
    "        v = new_f.iter_gen_key(new_s, 10)\n",
    "        s = utils.tree_to_str(v)\n",
    "        print(expr_double_paren(s), s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "298.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
