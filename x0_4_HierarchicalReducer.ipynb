{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delta Debugging is a method to reduce failure inducing inputs to their smallest\n",
    "required size that still induces the same failure. It was first formally introduced\n",
    "in the paper _Simplifying and Isolating Failure-Inducing Input_ by _Zeller_ and _Hildebrandt_.\n",
    "\n",
    "The idea of delta debugging is fairly simple. We start by partitioning the given input\n",
    "string, starting with two partitions â€“ which have a given partition length.\n",
    "Then, we check if any of these parts can be removed without removing the observed\n",
    "failure. If any of these can be removed, we remove all such parts of the given length.\n",
    "Once no such parts of the given length can be removed, we reduce the partition length\n",
    "by two, and do the same process again. This obtains us the 1-minimal failure causing\n",
    "string where removal of even a single character will remove the observed failure.\n",
    "\n",
    "## Delta Debugging\n",
    "\n",
    "### remove_check_each_fragment()\n",
    "\n",
    "Given a partition length, we want to split the string into\n",
    "that many partitions, remove each partition one at a time from the\n",
    "string, and check if for any of them, the `causal()` succeeds. If it\n",
    "succeeds for any, then we can skip that section of the string.\n",
    "\n",
    "**Note.** A more detailed treatment of delta debugging can be found [here](https://rahul.gopinath.org/post/2019/12/03/ddmin/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_check_each_fragment(instr, part_len, causal):\n",
    "    pre = ''\n",
    "    for i in range(0, len(instr), part_len):\n",
    "        stitched =  pre + instr[i+part_len:]\n",
    "        if not causal(stitched):\n",
    "             pre = pre + instr[i:i+part_len]\n",
    "    return pre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reason this function is split from the main function unlike in the\n",
    "original implementation of `ddmin`. The function `remove_check_each_fragment`\n",
    "obeys the contract that any string returned by it obeys the contract represented\n",
    "by the `causal` function. This means that any test case that is produced by\n",
    "`remove_check_each_fragment` will reproduce the specified behavior, and can be\n",
    "used for other computations. For example, one may use it for evaluating test\n",
    "reduction slippage, or for finding other reductions.\n",
    "\n",
    "\n",
    "### ddmin()\n",
    "\n",
    "The main function. We start by the smallest number of partitions -- 2.\n",
    "Then, we check by removing each fragment for success. If removing one\n",
    "fragment succeeds, we change the current string to the string without that\n",
    "fragment. So, we remove all fragments that can be removed in that partition\n",
    "size.\n",
    "If none of the fragments could be removed, then we reduce the partition length\n",
    "by half.\n",
    "If the partition cannot be halved again (i.e, the last partition length was\n",
    "one) or the string has become empty, we stop the iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ddmin(cur_str, causal_fn):\n",
    "    part_len = len(cur_str) // 2\n",
    "    while part_len and cur_str:\n",
    "        _str = remove_check_each_fragment(cur_str, part_len, causal_fn)\n",
    "        if _str == cur_str:\n",
    "            part_len = part_len // 2\n",
    "        cur_str = _str\n",
    "\n",
    "    return cur_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(s):\n",
    "    print(\"%s %d\" % (s, len(s)))\n",
    "    return set('()') <= set(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import random, string\n",
    "    inputstring = ''.join(random.choices(string.digits +\n",
    "                          string.ascii_letters +\n",
    "                          string.punctuation, k=1024))\n",
    "    print(inputstring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    assert test(inputstring)\n",
    "    solution = ddmin(inputstring, test)\n",
    "    print(solution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, pure _ddmin_ is limited when it comes to structured inputs.\n",
    "One of the problems with the unstructured version of `ddmin()` above is that\n",
    "it assumes that parts of the inputs can be cut away, while still retaining\n",
    "validity in that it will actually reach the testing function. This, however,\n",
    "may not be a reasonable assumption especially in the case of structured\n",
    "inputs. The problem is that if you have a JSON `[{\"a\": null}]` that produces\n",
    "an error because the key value is `null`, `ddmin()` will try to partition it\n",
    "as `[{\"a\":` followed by `null}]` neither of which are valid. Further, any\n",
    "chunk removed from either parts are also likely to be invalid. Hence,\n",
    "`ddmin()` will not be able to proceed.\n",
    "\n",
    "# Hierarchical Delta Debugging\n",
    "\n",
    "The solution here is to go for a variant called *Hierarchical Delta Debugging*\n",
    "(Misherghi 2006).\n",
    "The basic idea is to first extract the parse tree (either by parsing the input\n",
    "using a grammar, or by either relying on a generator to generate the parse\n",
    "tree along with the input, or by extracting the parse tree in-flight from the\n",
    "target program if it supports it), and then try and apply `ddmin()` on each\n",
    "level of the derivation tree. Another notable improvement was invented by\n",
    "Herfert et al. (Herfert 2017).\n",
    "In this paper, the authors describe a simple strategy: Try and replace a given\n",
    "node by one of its child nodes. This works reasonably well for inputs that are\n",
    "context sensitive such as programs that can trigger bugs in compilers.\n",
    "A final algorithm is Perses (Sun 2018) which uses the program grammar\n",
    "directly to avoid creating invalid nodes. In this post, I will explain a simplified\n",
    "variant of the algorithm of *Perses*. I note that a similar idea was explored by\n",
    "Pike in 2014 (Pike 2014) albeit for data structures.\n",
    "\n",
    "The basic idea of Perses is to try and replace a parent node by either an\n",
    "empty expansion (so that the corresponding string fragment can be deleted)\n",
    "or by a child node of the same nonterminal. To accomplish the first (that is,\n",
    "to be able to use empty expansions), Perses first transofrms the given grammar\n",
    "to a *Perses normal form*. We can however skip that portion if we are careful\n",
    "and use a grammar where we explicitly use nonterminals that can have an empty\n",
    "expansion. In this post, I only explain the *perses reduction* algorithm.\n",
    "\n",
    "**Note.** A more detailed treatment of hierarchical delta debugging can be found [here](https://rahul.gopinath.org/post/2019/12/04/hdd/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicate\n",
    "For the delta debug algorithm to work, we need a predicate. However, unlike\n",
    "*ddmin* which works with simple pass or fail status, we need a bit more\n",
    "sophistication here. The reason is that in many cases, the newly constructed\n",
    "strings we generate may also be *invalid*. For example, a particular program\n",
    "may produce a core dump from the C compiler. Hence, the core dump is the\n",
    "`pass` for the predicate. On the other hand, deleting the declaration of a\n",
    "variable may result in a compilation error. This is different from a simple\n",
    "`fail` which is no coredump on a semantically valid program. To capture these\n",
    "different conditions, we define new return values for the predicate.\n",
    "\n",
    "There can be four outcomes when an input is executed:\n",
    "* Success (failure condition reproduced)\n",
    "* Failed (failure condition not reproduced)\n",
    "* Invalid (Did not reach failure condition  possibly semantically invalid)\n",
    "* Timeout (equivalent to Failed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import copy\n",
    "\n",
    "class PRes(str, Enum):\n",
    "    success = 'SUCCESS'\n",
    "    failed = 'FAILED'\n",
    "    invalid = 'INVALID'\n",
    "    timeout = 'TIMEOUT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define our predicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def expr_double_paren(inp):\n",
    "    if re.match(r'.*[(][(].*[)][)].*', inp):\n",
    "        return PRes.success\n",
    "    return PRes.failed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(expr_double_paren('((1))'))\n",
    "    print(expr_double_paren('(1)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us define an input, and parse it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now define a grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipynb.fs.full.x0_1_Grammars as grammars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    import ipynb.fs.full.x0_3_Parser as parser\n",
    "    \n",
    "    my_input = '1+((2*3/4))'\n",
    "    expr_parser = parser.LL1Parser(grammars.EXPR_GRAMMAR)\n",
    "    parsed_expr = list(expr_parser.parse_on(my_input, grammars.EXPR_START))[0]\n",
    "    utils.display_tree(parsed_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perses algorithm is as follows: We start the root, and recursively go down\n",
    "the child nodes. For each node, we check if that node can be replaced by a\n",
    "subtree with the same nonterminal, and still reproduce the failure, and find\n",
    "the smallest such tree (length determined by number of leaves).\n",
    "\n",
    "Since this procedure can result in multiple trees, the tree to work on is\n",
    "chosen based on a priority queue where the priority is given to the smallest\n",
    "tree.\n",
    "\n",
    "The particular node chosen to replace the current node is determined based\n",
    "first on its number of leaf nodes, and then on its rank in a priority queue,\n",
    "where the priority is determined by the depth of the subtree from the current\n",
    "node. That is, a child gets priority over a grand child.\n",
    "We first have to define a way to address a specific node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    t = parsed_expr[1][0][1][2][1][0]\n",
    "    print(t)\n",
    "    utils.display_tree(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the path, we simply use a list of numbers indicating the child node. For example, in the above, the path would be [0, 2, 0]\n",
    "Given a path, get_child() will simply return the node at the path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_child(tree, path):\n",
    "    if not path: return tree\n",
    "    cur, *path = path\n",
    "    return get_child(tree[1][cur], path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    te = get_child(parsed_expr, [0, 2, 0])\n",
    "    utils.display_tree(te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a way to replace one node with another. This is done by replace_path()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_path(tree, path, new_node=None):\n",
    "    if new_node is None: new_node = []\n",
    "    if not path: return copy.deepcopy(new_node)\n",
    "    cur, *path = path\n",
    "    name, children, *rest = tree\n",
    "    new_children = []\n",
    "    for i,c in enumerate(children):\n",
    "        if i == cur:\n",
    "            nc = replace_path(c, path, new_node)\n",
    "        else:\n",
    "            nc = c\n",
    "        if nc:\n",
    "            new_children.append(nc)\n",
    "    return (name, new_children, *rest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    t = parsed_expr[1][0][1][2][1][0]\n",
    "    utils.display_tree(t)\n",
    "    te = replace_path(parsed_expr, [0, 2, 0], [])\n",
    "    utils.display_tree(te)\n",
    "    tn = replace_path(parsed_expr, [0, 2, 0], ('x',[]))\n",
    "    utils.display_tree(tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Priority queue\n",
    "\n",
    "For Perses reduction, one needs a way to count the number of leaf nodes to\n",
    "determine the priority of a node. This is done by count_leaves()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "def count_leaves(node):\n",
    "    name, children, *_ = node\n",
    "    if not children:\n",
    "        return 1\n",
    "    return sum(count_leaves(i) for i in children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(count_leaves(parsed_expr))\n",
    "    print(count_leaves(te))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a helper that simply counts the internal nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(node):\n",
    "    name, children, *_ = node\n",
    "    if not children:\n",
    "        return 0\n",
    "    return sum(count_nodes(i) for i in children) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(count_nodes(parsed_expr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to maintain a priority queue of the [(tree, path)]. The\n",
    "essential idea is to prioritize the items first by the number of leaves in  the\n",
    "full tree (that is, the smallest tree that we have currently gets priority),\n",
    "then next by the number of leaves in the node pointed to by path, and finally,\n",
    "tie break by the insertion order (ecount)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ecount = 0\n",
    "\n",
    "def add_to_pq(tup, q):\n",
    "    global ecount\n",
    "    dtree, F_path = tup\n",
    "    stree = get_child(dtree, F_path)\n",
    "    n =  count_leaves(dtree)\n",
    "    m =  count_leaves(stree)\n",
    "    # heap smallest first\n",
    "    heapq.heappush(q, (n, m, -ecount, tup))\n",
    "    ecount += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define another helper function nt_group() that groups all nonterminals that have the same name. These are used to determine the nodes that can be used to replace one node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nt_group(tree, all_nodes=None):\n",
    "    if all_nodes is None: all_nodes = {}\n",
    "    name, children, *_ = tree\n",
    "    if not utils.is_nt(name): return\n",
    "    all_nodes.setdefault(name, []).append(tree)\n",
    "    for c in children:\n",
    "        nt_group(c, all_nodes)\n",
    "    return all_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    gp = nt_group(te)\n",
    "    for key in gp:\n",
    "        print(key)\n",
    "        for node in gp[key]:\n",
    "            print(utils.tree_to_str(node))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the compatible nodes? These are all the nodes that have the same nonterminal name, and is a descendent of the current node. Further, if the nonterminal allows empty node, then this is the first in the list. This is defined by compatible_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compatible_nodes(tree, grammar):\n",
    "    key, children, *_ = tree\n",
    "    # Here is the first choice. Do we restrict ourselves to only children of the tree\n",
    "    # or do we allow all nodes in the original tree? given in all_nodes?\n",
    "    lst = nt_group(tree)\n",
    "    node_lst = [(i, n) for i,n in enumerate(lst[key])]\n",
    "\n",
    "    # insert empty if the grammar allows it as the first element\n",
    "    if [] in grammar[key]: node_lst.insert(0, (-1, (key, [])))\n",
    "    return node_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print(get_child(te, [0]))\n",
    "    print(compatible_nodes(get_child(te, [0]), grammars.EXPR_GRAMMAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some programming languages have tokens which are first level lexical elements. The parser is often defined using the lexer tokens. We do not want to try to reduce tokens further. So we define a way to identify them (we have to keep in mind when we produce grammars). For now, we assume the ANTLR convention of identifying tokens by uppercase letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_token(val):\n",
    "    assert val != '<>'\n",
    "    assert (val[0], val[-1]) == ('<', '>')\n",
    "    if val[1].isupper(): return True\n",
    "    #if val[1] == '_': return val[2].isupper() # token derived.\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified Perses reduction\n",
    "We finally define the reduction algorithm. The implementation of Perses is given in reduction(). The essential idea is as follows:\n",
    "\n",
    "1. We have a priority queue of (tree, path_to_node) structures, where node is a node within the tree.\n",
    "  * The highest priority is given to the smallest tree.\n",
    "  * With in the nodes in the same tree, priority is given to nodes with smallest number of leaves\n",
    "  * In case of tie break, the shallowest subnode gets the highest priority (i.e child has higher priority over grand child, and empty node has the highest priority since it is a peer of the current node).\n",
    "2. We pick each nodes, and find compatible subnodes that reproduce the failure.\n",
    "3. Each compatible node and the corresponding tree is put back into the priority queue.\n",
    "4. If no child nodes were found that could replace the current node, then we add each children with the current tree into the priority queue. (If we had to recurse into the child nodes, then the next tree that will get picked will be a different tree.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_reduction(tree, grammar, predicate):\n",
    "    first_tuple = (tree, [])\n",
    "    p_q = []\n",
    "    add_to_pq(first_tuple, p_q)\n",
    "\n",
    "    ostr = utils.tree_to_str(tree)\n",
    "    assert predicate(ostr) == PRes.success\n",
    "    failed_set = {ostr: True}\n",
    "\n",
    "    min_tree, min_tree_size = tree, count_leaves(tree)\n",
    "    while p_q:\n",
    "        # extract the tuple\n",
    "        _n, _m, _ec, (dtree, F_path) = heapq.heappop(p_q)\n",
    "        stree = get_child(dtree, F_path)\n",
    "        skey, schildren = stree\n",
    "        found = False\n",
    "        # we now want to replace stree with alternate nodes.\n",
    "        for i, node in compatible_nodes(stree, grammar):\n",
    "            # replace with current (copy).\n",
    "            ctree = replace_path(dtree, F_path, node)\n",
    "            if ctree is None: continue # same node\n",
    "            v = utils.tree_to_str(ctree)\n",
    "            if v in failed_set: continue\n",
    "            failed_set[v] = predicate(v) # we ignore PRes.invalid results\n",
    "            if failed_set[v] == PRes.success:\n",
    "                found = True\n",
    "                ctree_size = count_leaves(ctree)\n",
    "                if ctree_size < min_tree_size: min_tree, min_tree_size = ctree, ctree_size\n",
    "\n",
    "                if v not in failed_set:\n",
    "                    print(v)\n",
    "                t = (ctree, F_path)\n",
    "                assert get_child(ctree, F_path) is not None\n",
    "                add_to_pq(t, p_q)\n",
    "\n",
    "        # The CHOICE here is that we explore the children if and only if we fail\n",
    "        # to find a node that can replace the current\n",
    "        if found: continue\n",
    "        if is_token(skey): continue # do not follow children TOKEN optimization\n",
    "        for i, child in enumerate(schildren):\n",
    "            if not utils.is_nt(child[0]): continue\n",
    "            assert get_child(tree=dtree, path=F_path + [i]) is not None\n",
    "            t = (dtree, F_path + [i])\n",
    "            add_to_pq(t, p_q)\n",
    "    return min_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    er = hierarchical_reduction(parsed_expr, grammars.EXPR_GRAMMAR, expr_double_paren)\n",
    "    print(utils.tree_to_str(er))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A question for you. Why doesn't this work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    parsed_expr = list(expr_parser.parse_on('1+((2*3/4))*5', grammars.EXPR_START))[0]\n",
    "    reduced_expr_tree = hierarchical_reduction(parsed_expr, grammars.EXPR_GRAMMAR, expr_double_paren)\n",
    "    print(utils.tree_to_str(reduced_expr_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    utils.display_tree(reduced_expr_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that we don't reduce `<factor> * <term>` to `<factor>`. This can be done if there exist rules that contain a subset of tokens of the current rule. However, in the general case, this requires random generation of nonterminals that are not available in the current rule, which can be costly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
