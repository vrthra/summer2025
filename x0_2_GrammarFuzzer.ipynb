{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grammar Fuzzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a fairly simple grammar fuzzer. For a detailed treatment, please see my blog [here](https://rahul.gopinath.org/post/2019/05/28/simplefuzzer-01/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** If this fails, you have not installed graphviz properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import ipynb.fs.full.x0_1_Grammars as grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We allow the grammar to contain a few lexical definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ASCII_MAP = {\n",
    "        '[__WHITESPACE__]': string.whitespace,\n",
    "        '[__DIGIT__]': string.digits,\n",
    "        '[__ASCII_LOWER__]': string.ascii_lowercase,\n",
    "        '[__ASCII_UPPER__]': string.ascii_uppercase,\n",
    "        '[__ASCII_PUNCT__]': string.punctuation,\n",
    "        '[__ASCII_LETTER__]': string.ascii_letters,\n",
    "        '[__ASCII_ALPHANUM__]': string.ascii_letters + string.digits,\n",
    "        '[__ASCII_PRINTABLE__]': string.printable\n",
    "        }\n",
    "FUZZRANGE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The fuzzer interface\n",
    "Here is the fuzzer interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fuzzer:\n",
    "    def __init__(self, grammar):\n",
    "        self.grammar = grammar\n",
    "\n",
    "    def fuzz(self, key='<start>', max_num=None, max_depth=None):\n",
    "        raise NotImplemented()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limit Fuzer\n",
    "The initialiation. It takes only the grammar to initialize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(Fuzzer):\n",
    "    def __init__(self, grammar):\n",
    "        super().__init__(grammar)\n",
    "        self.key_cost = {}\n",
    "        self.cost = self.compute_cost(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A grammar may contain recursive rules. For example, in the `EXPR_GRAMMAR` defined above, `<expr>` contains a rules that reference `<term>`. The definition of `<term>` contains rules that refer `<factor>` and `<factor>` definition in turn refers to `<expr>`. The problem here is that, since we rely on random choice to select the rule to expand, the recursion may continue much beyond our budget. Hence, we need a way to restrict the recursion. This is done by usign the *cost*. The cost of a recursive nonterminal is infinite. The idea is that we first identify all the non-recursive rules in a nonterminal definition. Once we exeed a given maximum depth, only choose rules to expand from the non-recursive set.\n",
    "\n",
    "###  Cost\n",
    "The cost of a nonterminal symbol is the minimum cost of the rules that define it. A terminal symbol costs a unit. The cost of a rule is the maximum cost of all the symbols that are in the rule.\n",
    "\n",
    "So, we first find the cost of expansion for each grammar rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(LimitFuzzer):\n",
    "    def compute_cost(self, grammar):\n",
    "        cost = {}\n",
    "        for k in grammar:\n",
    "            cost[k] = {}\n",
    "            for rule in grammar[k]:\n",
    "                cost[k][str(rule)] = self.expansion_cost(grammar, rule, set())\n",
    "            if len(grammar[k]):\n",
    "                assert len([v for v in cost[k] if v != float('inf')]) > 0\n",
    "        return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Symbol cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(LimitFuzzer):\n",
    "    def symbol_cost(self, grammar, symbol, seen):\n",
    "        if symbol in self.key_cost: return self.key_cost[symbol]\n",
    "        if symbol in seen:\n",
    "            self.key_cost[symbol] = float('inf')\n",
    "            return float('inf')\n",
    "        v = min((self.expansion_cost(grammar, rule, seen | {symbol})\n",
    "                    for rule in grammar.get(symbol, [])), default=0)\n",
    "        self.key_cost[symbol] = v\n",
    "        return v\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(LimitFuzzer):\n",
    "    def expansion_cost(self, grammar, tokens, seen):\n",
    "        return max((self.symbol_cost(grammar, token, seen)\n",
    "                    for token in tokens if token in grammar), default=0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fuzz()\n",
    "Generating keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimitFuzzer(LimitFuzzer):\n",
    "    def nonterminals(self, rule):\n",
    "        return [t for t in rule if utils.is_nt(t)]\n",
    "\n",
    "    def iter_gen_key(self, key, max_depth):\n",
    "        def get_def(t):\n",
    "            if t in ASCII_MAP:\n",
    "                return [random.choice(ASCII_MAP[t]), []]\n",
    "            elif t and t[-1] == '+' and t[0:-1] in ASCII_MAP:\n",
    "                num = random.randrange(FUZZRANGE) + 1\n",
    "                val = [random.choice(ASCII_MAP[t[0:-1]]) for i in range(num)]\n",
    "                return [''.join(val), []]\n",
    "            elif utils.is_nt(t):\n",
    "                return [t, None]\n",
    "            else:\n",
    "                return [t, []]\n",
    "\n",
    "        cheap_grammar = {}\n",
    "        for k in self.cost:\n",
    "            rules = self.grammar[k]\n",
    "            if rules:\n",
    "                min_cost = min([self.cost[k][str(r)] for r in rules])\n",
    "                cheap_grammar[k] = [r for r in self.grammar[k] if self.cost[k][str(r)] == min_cost]\n",
    "            else:\n",
    "                cheap_grammar[k] = [] # (No rules found)\n",
    "\n",
    "        root = [key, None]\n",
    "        queue = [(0, root)]\n",
    "        while queue:\n",
    "            (depth, item), *queue = queue\n",
    "            key = item[0]\n",
    "            if item[1] is not None: continue\n",
    "            grammar = self.grammar if depth < max_depth else cheap_grammar\n",
    "            chosen_rule = random.choice(grammar[key])\n",
    "            expansion = [get_def(t) for t in chosen_rule]\n",
    "            item[1] = expansion\n",
    "            for t in expansion: queue.append((depth+1, t))\n",
    "\n",
    "        return root\n",
    "    \n",
    "    def gen_key(self, key, depth, max_depth):\n",
    "        if key in ASCII_MAP:\n",
    "            return (random.choice(ASCII_MAP[key]), [])\n",
    "        if key and key[-1] == '+' and key[0:-1] in ASCII_MAP:\n",
    "            m = random.randrange(FUZZRANGE) + 1\n",
    "            return (''.join([random.choice(ASCII_MAP[key[0:-1]]) for i in range(m)]), [])\n",
    "        if key not in self.grammar: return (key, [])\n",
    "        if depth > max_depth:\n",
    "            clst = sorted([(self.cost[key][str(rule)], rule) for rule in self.grammar[key]])\n",
    "            rules = [r for c,r in clst if c == clst[0][0]]\n",
    "        else:\n",
    "            rules = self.grammar[key]\n",
    "        v = self.gen_rule(random.choice(rules), depth+1, max_depth)\n",
    "        return (key, v)\n",
    "    \n",
    "    def gen_rule(self, rule, depth, max_depth):\n",
    "        return [self.gen_key(token, depth, max_depth) for token in rule]\n",
    "\n",
    "    def fuzz(self, key='<start>', max_depth=10):\n",
    "        return utils.tree_to_str(self.iter_gen_key(key=key, max_depth=max_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    lf = LimitFuzzer(grammars.EXPR_GRAMMAR)\n",
    "    my_tree = lf.iter_gen_key(grammars.EXPR_START,max_depth=10)\n",
    "    for i in range(10):\n",
    "        v = lf.fuzz(grammars.EXPR_START)\n",
    "        print(repr(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    utils.display_tree(my_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compiled Grammar Fuzzer\n",
    "\n",
    "This grammar fuzzer is described in the paper [*Building Fast Fuzzers*](https://rahul.gopinath.org/publications/2019/11/18/arxiv-building/). The idea is to compile a grammar definition to the corresponding source code. Each nonterminal symbol becomes a procedure. First we define a few helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Fuzzer(LimitFuzzer):\n",
    "    def add_indent(self, string, indent):\n",
    "        return '\\n'.join([indent + i for i in string.split('\\n')])\n",
    "\n",
    "    # used for escaping inside strings\n",
    "    def esc(self, t):\n",
    "        t = t.replace('\\\\', '\\\\\\\\')\n",
    "        t = t.replace('\\n', '\\\\n')\n",
    "        t = t.replace('\\r', '\\\\r')\n",
    "        t = t.replace('\\t', '\\\\t')\n",
    "        t = t.replace('\\b', '\\\\b')\n",
    "        t = t.replace('\\v', '\\\\v')\n",
    "        t = t.replace('\"', '\\\\\"')\n",
    "        return t\n",
    "    \n",
    "    def esc_char(self, t):\n",
    "        assert len(t) == 1\n",
    "        t = t.replace('\\\\', '\\\\\\\\')\n",
    "        t = t.replace('\\n', '\\\\n')\n",
    "        t = t.replace('\\r', '\\\\r')\n",
    "        t = t.replace('\\t', '\\\\t')\n",
    "        t = t.replace('\\b', '\\\\b')\n",
    "        t = t.replace('\\v', '\\\\v')\n",
    "        t = t.replace(\"'\", \"\\\\'\")\n",
    "        return t\n",
    "\n",
    "    def k_to_s(self, k): return k[1:-1].replace('-', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cheap grammar compilation\n",
    "The complication is that we need to curtail the recursion. Hence, we define a cheap grammar that does not contain recursion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Fuzzer(F1Fuzzer):\n",
    "    def cheap_grammar(self):\n",
    "        cheap_grammar = {}\n",
    "        for k in self.cost:\n",
    "            rules = self.grammar[k]\n",
    "            if rules:\n",
    "                min_cost = min([self.cost[k][str(r)] for r in rules])\n",
    "                cheap_grammar[k] = [r for r in self.grammar[k] if self.cost[k][str(r)] == min_cost]\n",
    "            else:\n",
    "                cheap_grammar[k] = [] # (No rules found)\n",
    "        return cheap_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translation\n",
    "Translating the nonterminals of the cheap grammar is simple because there is no recursion. We simply choose a random rule to expand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Fuzzer(F1Fuzzer):\n",
    "    def gen_rule_src_cheap(self, rule, key, i, grammar):\n",
    "        res = []\n",
    "        for token in rule:\n",
    "            if token in grammar:\n",
    "                res.append('''\\\n",
    "gen_%s_cheap()''' % (self.k_to_s(token)))\n",
    "            else:\n",
    "                res.append('''\\\n",
    "result.append(\"%s\")''' % self.esc(token))\n",
    "        return '\\n'.join(res)\n",
    "\n",
    "\n",
    "    def gen_alt_src_cheap(self, key, grammar):\n",
    "        rules = grammar[key]\n",
    "        result = []\n",
    "        result.append('''\n",
    "def gen_%(name)s_cheap():\n",
    "    val = random.randrange(%(nrules)s)''' % {\n",
    "            'name':self.k_to_s(key),\n",
    "            'nrules':len(rules)})\n",
    "        for i, rule in enumerate(rules):\n",
    "            result.append('''\\\n",
    "    if val == %d:\n",
    "%s\n",
    "        return''' % (i, self.add_indent(self.gen_rule_src_cheap(rule, key, i, grammar),'        ')))\n",
    "        return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For recursive grammars, we need to verify that the depth of recursion is not beyond what is specified. If it has gone beyond the maximum specified depth, we expand the cheap grammar instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Fuzzer(F1Fuzzer):\n",
    "    def gen_rule_src(self, rule, key, i, grammar):\n",
    "        res = []\n",
    "        for token in rule:\n",
    "            if token in grammar:\n",
    "                res.append('''\\\n",
    "gen_%s(max_depth, next_depth)''' % (self.k_to_s(token)))\n",
    "            else:\n",
    "                res.append('''\\\n",
    "result.append(\"%s\")''' % self.esc(token))\n",
    "        return '\\n'.join(res)\n",
    "\n",
    "    def gen_alt_src(self, key, grammar):\n",
    "        rules = grammar[key]\n",
    "        result = []\n",
    "        result.append('''\n",
    "def gen_%(name)s(max_depth, depth=0):\n",
    "    next_depth = depth + 1\n",
    "    if depth > max_depth:\n",
    "        gen_%(name)s_cheap()\n",
    "        return\n",
    "    val = random.randrange(%(nrules)s)''' % {\n",
    "            'name':self.k_to_s(key),\n",
    "            'nrules':len(rules)})\n",
    "        for i, rule in enumerate(rules):\n",
    "            result.append('''\\\n",
    "    if val == %d:\n",
    "%s\n",
    "        return''' % (i, self.add_indent(self.gen_rule_src(rule, key, i, grammar),'        ')))\n",
    "        return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1Fuzzer(F1Fuzzer):\n",
    "    def gen_main_src(self):\n",
    "        return '''\n",
    "import random\n",
    "result = []\n",
    "def start(max_depth):\n",
    "    gen_start(max_depth)\n",
    "    v = ''.join(result)\n",
    "    result.clear()\n",
    "    return v\n",
    "        '''\n",
    "\n",
    "    def gen_fuzz_src(self):\n",
    "        result = []\n",
    "        cheap_grammar = self.cheap_grammar()\n",
    "        for key in cheap_grammar:\n",
    "            result.append(self.gen_alt_src_cheap(key, cheap_grammar))\n",
    "        for key in self.grammar:\n",
    "            result.append(self.gen_alt_src(key, self.grammar))\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        result = [self.gen_fuzz_src(),\n",
    "                  self.gen_main_src()]\n",
    "        return ''.join(result)\n",
    "    \n",
    "    def fuzzer(self, name):\n",
    "        cf_src = self.fuzz_src()\n",
    "        return utils.load_src(cf_src, name + '_f1_fuzzer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    expr_fuzzer = F1Fuzzer(grammars.EXPR_GRAMMAR).fuzzer('expr_fuzzer')\n",
    "    for i in range(10):\n",
    "        v = expr_fuzzer.start(10)\n",
    "        print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Problem -- Recursion\n",
    "\n",
    "A problem with the compiled grammar fuzzer is that it relies on recursion, and Python limits the recursion depth arbitrarily (starting with just 1000). Hence, we ned a solution that allows us to go past that depth.\n",
    "\n",
    "The idea here is to exploit the generator infrastructure in Python. We keep returning nested generators and use a trampoline to execute the generators returned.\n",
    "\n",
    "Each previous function call is transformed into a `yield` and the generator thus produced is returned.\n",
    "\n",
    "See [here](https://rahul.gopinath.org/post/2022/04/17/python-iterative-copy/) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1CPSFuzzer(F1Fuzzer):\n",
    "    def gen_rule_src_cheap(self, rule, key, i, grammar):\n",
    "        res = []\n",
    "        for token in rule:\n",
    "            if token in grammar:\n",
    "                res.append('''\\\n",
    "yield gen_%s_cheap()''' % (self.k_to_s(token)))\n",
    "            else:\n",
    "                res.append('''\\\n",
    "result.append(\"%s\")''' % self.esc(token))\n",
    "        return '\\n'.join(res)\n",
    "\n",
    "\n",
    "    def gen_alt_src_cheap(self, key, grammar):\n",
    "        rules = grammar[key]\n",
    "        result = []\n",
    "        result.append('''\n",
    "def gen_%(name)s_cheap():\n",
    "    val = random.randrange(%(nrules)s)''' % {\n",
    "            'name':self.k_to_s(key),\n",
    "            'nrules':len(rules)})\n",
    "        for i, rule in enumerate(rules):\n",
    "            result.append('''\\\n",
    "    if val == %d:\n",
    "%s\n",
    "        return''' % (i, self.add_indent(self.gen_rule_src_cheap(rule, key, i, grammar),'        ')))\n",
    "        return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1CPSFuzzer(F1CPSFuzzer):\n",
    "    def gen_rule_src(self, rule, key, i, grammar):\n",
    "        res = []\n",
    "        for token in rule:\n",
    "            if token in grammar:\n",
    "                res.append('''\\\n",
    "yield gen_%s(max_depth, next_depth)''' % (self.k_to_s(token)))\n",
    "            else:\n",
    "                res.append('''\\\n",
    "result.append(\"%s\")''' % self.esc(token))\n",
    "        return '\\n'.join(res)\n",
    "\n",
    "    def gen_alt_src(self, key, grammar):\n",
    "        rules = grammar[key]\n",
    "        result = []\n",
    "        result.append('''\n",
    "def gen_%(name)s(max_depth, depth=0):\n",
    "    next_depth = depth + 1\n",
    "    if depth > max_depth:\n",
    "        yield gen_%(name)s_cheap()\n",
    "        return\n",
    "    val = random.randrange(%(nrules)s)''' % {\n",
    "            'name':self.k_to_s(key),\n",
    "            'nrules':len(rules)})\n",
    "        for i, rule in enumerate(rules):\n",
    "            result.append('''\\\n",
    "    if val == %d:\n",
    "%s\n",
    "        return''' % (i, self.add_indent(self.gen_rule_src(rule, key, i, grammar),'        ')))\n",
    "        return '\\n'.join(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generators thus returned is added to a stack, and the generator at the top of the stack is extracted, and continued."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1CPSFuzzer(F1CPSFuzzer):\n",
    "    def gen_main_src(self):\n",
    "        return '''\n",
    "def cpstrampoline(gen):\n",
    "    stack = [gen]\n",
    "    ret = None\n",
    "    while stack:\n",
    "        try:\n",
    "            value, ret = ret, None\n",
    "            res = stack[-1].send(value)\n",
    "            if res is not None:\n",
    "                stack.append(res)\n",
    "        except StopIteration as e:\n",
    "            stack.pop()\n",
    "            ret = e.value\n",
    "    return ret\n",
    "\n",
    "import random\n",
    "result = []\n",
    "def start(max_depth):\n",
    "    cpstrampoline(gen_start(max_depth))\n",
    "    v = ''.join(result)\n",
    "    result.clear()\n",
    "    return v\n",
    "        '''\n",
    "\n",
    "    def gen_fuzz_src(self):\n",
    "        result = []\n",
    "        cheap_grammar = self.cheap_grammar()\n",
    "        for key in cheap_grammar:\n",
    "            result.append(self.gen_alt_src_cheap(key, cheap_grammar))\n",
    "        for key in self.grammar:\n",
    "            result.append(self.gen_alt_src(key, self.grammar))\n",
    "        return '\\n'.join(result)\n",
    "\n",
    "    def fuzz_src(self, key='<start>'):\n",
    "        result = [self.gen_fuzz_src(),\n",
    "                  self.gen_main_src()]\n",
    "        return ''.join(result)\n",
    "    \n",
    "    def fuzzer(self, name):\n",
    "        cf_src = self.fuzz_src()\n",
    "        return utils.load_src(cf_src, name + '_f1_fuzzer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    expr_fuzzer = F1CPSFuzzer(grammars.EXPR_GRAMMAR).fuzzer('expr_fuzzer')\n",
    "    for i in range(10):\n",
    "        v = expr_fuzzer.start(10)\n",
    "        print(repr(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
